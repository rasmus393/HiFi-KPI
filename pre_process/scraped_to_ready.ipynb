{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1884049 JSON files.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import glob\n",
    "\n",
    "folder_path = r\"\\\\cph3\\academics\\Parsed\"\n",
    "\n",
    "# Get all JSON files in the folder\n",
    "json_files = glob.glob(os.path.join(folder_path, \"*.json\"))\n",
    "\n",
    "# Load JSON data into a list\n",
    "data = []\n",
    "for file in json_files:\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data += json.load(f)\n",
    "        #data_list.append(data)\n",
    "\n",
    "print(f\"Loaded {len(data)} JSON files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 1523481\n",
      "Validation set size: 208542\n",
      "Test set size: 152026\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "validation_start = datetime(2023, 10, 31)\n",
    "test_start = datetime(2024, 3, 1)\n",
    "\n",
    "# Convert filing_date to datetime and split\n",
    "train, validation, test = [], [], []\n",
    "\n",
    "for item in data:\n",
    "    filing_date = datetime.strptime(item['filing_date'], '%Y%m%d%H%M%S')\n",
    "    if filing_date < validation_start:\n",
    "        train.append(item)\n",
    "    elif validation_start <= filing_date < test_start:\n",
    "        validation.append(item)\n",
    "    else:\n",
    "        test.append(item)\n",
    "\n",
    "# Output results\n",
    "print(f\"Training set size: {len(train)}\")\n",
    "print(f\"Validation set size: {len(validation)}\")\n",
    "print(f\"Test set size: {len(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rasmus.jensen\\AppData\\Local\\Temp\\ipykernel_29008\\3932241198.py:38: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  val_df = pd.concat(val_set)\n",
      "C:\\Users\\rasmus.jensen\\AppData\\Local\\Temp\\ipykernel_29008\\3932241198.py:39: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_df = pd.concat(test_set)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "df = pd.DataFrame((validation + test))\n",
    "\n",
    "df['filing_date'] = pd.to_datetime(df['filing_date'])\n",
    "\n",
    "company_groups = df.groupby('company_name')\n",
    "\n",
    "val_set = []\n",
    "test_set = []\n",
    "\n",
    "val_proportion = 0.5  # 50% for validation, 50% for testing\n",
    "\n",
    "for company_name, group in company_groups:\n",
    "    if len(group) > 1:\n",
    "        # Sort records by date within the group\n",
    "        group = group.sort_values(by='filing_date')\n",
    "        \n",
    "        # this means that the earlier filings are still in the val set instead of the test set.\n",
    "        split_index = int(len(group) * val_proportion)\n",
    "        \n",
    "        # Assign earlier records to validation and later records to test\n",
    "        val_sample = group.iloc[:split_index]\n",
    "        test_sample = group.iloc[split_index:]\n",
    "    else:\n",
    "        # Randomly assign the single record to either validation or test\n",
    "        if random.random() < 0.5:\n",
    "            val_sample = group\n",
    "            test_sample = pd.DataFrame(columns=group.columns)  # Empty test sample\n",
    "        else:\n",
    "            test_sample = group\n",
    "            val_sample = pd.DataFrame(columns=group.columns)  # Empty validation sample\n",
    "\n",
    "    val_set.append(val_sample)\n",
    "    test_set.append(test_sample)\n",
    "\n",
    "val_df = pd.concat(val_set)\n",
    "test_df = pd.concat(test_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated val_dataset has 171397 entries.\n",
      "Updated test_dataset has 189171 entries.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "train_df = pd.DataFrame(train)\n",
    "all_data = pd.concat([train_df, val_df, test_df], axis=0)\n",
    "val_test = pd.concat([val_df, test_df], axis=0)\n",
    "all_data['filing_date'] = pd.to_datetime(all_data['filing_date'])\n",
    "\n",
    "first_appearance = all_data.groupby('company_name')['filing_date'].min()\n",
    "\n",
    "# Define the threshold date\n",
    "threshold_date = datetime(2023, 10, 31)\n",
    "\n",
    "# Step 3: Identify companies introduced after the threshold date\n",
    "new_companies = first_appearance[first_appearance > threshold_date].index.tolist()\n",
    "\n",
    "# Step 4: Filter `val_test` to include only entries from the new companies\n",
    "filtered_data = val_test[val_test['company_name'].isin(new_companies)]\n",
    "\n",
    "# Step 4: Build the new validation and test sets\n",
    "# Validation set excludes entries from new companies\n",
    "val_dataset = val_df[~val_df['company_name'].isin(new_companies)].copy()\n",
    "\n",
    "# Test set includes entries from new companies\n",
    "test_dataset = pd.concat([test_df, val_df[val_df['company_name'].isin(new_companies)].copy()])\n",
    "\n",
    "# Display the results\n",
    "print(f\"Updated val_dataset has {len(val_dataset)} entries.\")\n",
    "print(f\"Updated test_dataset has {len(test_dataset)} entries.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['text'] = test_df['text'].apply(lambda t: t.replace('\\xa0', ' '))\n",
    "val_df['text'] = val_df['text'].apply(lambda t: t.replace('\\xa0', ' '))\n",
    "train_df['text'] = train_df['text'].apply(lambda t: t.replace('\\xa0', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally, save the splits to JSON files\n",
    "train_df.to_json('train_rescraped.json', orient='records', indent=4, force_ascii=False)\n",
    "\n",
    "# Save the validation set\n",
    "val_dataset.to_json('validation_rescraped.json', orient='records', indent=4, force_ascii=False)\n",
    "\n",
    "# Save the test set\n",
    "test_dataset.to_json('test_rescraped.json', orient='records', indent=4, force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix timestamps to be ISO formatted.\n",
    "\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "def convert_timestamp_to_iso(data, date_key=\"filing_date\"):\n",
    "    \"\"\"\n",
    "    Recursively finds and converts various date formats (millisecond Unix \n",
    "    timestamps and 'YYYYMMDDHHMMSS' strings) to the ISO 8601 format.\n",
    "\n",
    "    Args:\n",
    "        data (dict or list): The JSON data to process.\n",
    "        date_key (str): The key of the date field to convert.\n",
    "\n",
    "    Returns:\n",
    "        The processed data with converted dates.\n",
    "    \"\"\"\n",
    "    if isinstance(data, dict):\n",
    "        for key, value in data.items():\n",
    "            if key == date_key:\n",
    "                iso_date = None\n",
    "                if isinstance(value, (int, float)):\n",
    "                    # Handle Unix timestamp in milliseconds\n",
    "                    try:\n",
    "                        timestamp_sec = value / 1000.0\n",
    "                        iso_date = datetime.fromtimestamp(timestamp_sec, tz=timezone.utc).isoformat()\n",
    "                    except (ValueError, OSError): # Handles out of range timestamps\n",
    "                        pass # Keep original value if conversion fails\n",
    "                elif isinstance(value, str):\n",
    "                    # Handle string-based dates\n",
    "                    try:\n",
    "                        # Attempt to parse 'YYYYMMDDHHMMSS' format\n",
    "                        if len(value) >= 14 and value.isdigit():\n",
    "                            dt_object = datetime.strptime(value[:14], '%Y%m%d%H%M%S')\n",
    "                            # Make it timezone-aware (assuming UTC) before formatting\n",
    "                            iso_date = dt_object.replace(tzinfo=timezone.utc).isoformat()\n",
    "                    except ValueError:\n",
    "                        # If parsing fails, it's not the expected string format.\n",
    "                        # We'll leave the original value as is.\n",
    "                        pass\n",
    "                \n",
    "                if iso_date:\n",
    "                    data[key] = iso_date\n",
    "            else:\n",
    "                # Recurse into nested dictionaries or lists\n",
    "                data[key] = convert_timestamp_to_iso(value, date_key)\n",
    "    elif isinstance(data, list):\n",
    "        # If the data is a list, iterate through its items\n",
    "        return [convert_timestamp_to_iso(item, date_key) for item in data]\n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_json_file(input_path, output_path):\n",
    "    \"\"\"\n",
    "    Reads a JSON file, converts the 'filing_date' to ISO format,\n",
    "    and saves it to a new file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Processing '{input_path}'...\")\n",
    "        # Open and load the original JSON file\n",
    "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        # Convert the dates in the loaded data\n",
    "        converted_data = convert_timestamp_to_iso(json_data)\n",
    "        # Save the updated data to the new file\n",
    "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(converted_data, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Successfully converted and saved to '{output_path}'\\n\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file '{input_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred while processing {input_path}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Configuration ---\n",
    "    # Define the base directory where your files are located\n",
    "    base_dir = r\"D:\\PhD\\Data\\HIFi-paper\\final_unit_converted\"\n",
    "\n",
    "    # List of filenames to process\n",
    "    filenames = [\"train.json\", \"validation.json\", \"test.json\"]\n",
    "    \n",
    "    # --- Execution ---\n",
    "    for filename in filenames:\n",
    "        input_file = f\"{base_dir}\\\\{filename}\"\n",
    "        # Create a new name for the output file, e.g., 'train_iso.json'\n",
    "        output_file = f\"{base_dir}\\\\{filename.replace('.json', '_iso.json')}\"\n",
    "        process_json_file(input_file, output_file)\n",
    "\n",
    "    print(\"All files have been processed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhD",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
